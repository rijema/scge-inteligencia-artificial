{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Equipe 01 - Motor classificador de tópicos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SmViIi2_cu0"
      },
      "source": [
        "# **Equipe 01 - Motor classificador de tópicos**\n",
        "---\n",
        "Escola Politécnica da Universidade de Pernambuco - UPE/POLI\n",
        "\n",
        "Disciplina: Inteligência Artificial/ 2020.2\n",
        "\n",
        "Integrantes\n",
        "- Matheus Phelipe (mpap@ecomp.poli.br)\n",
        "- Murilo Stoldoni (mcs2@ecomp.poli.br)\n",
        "- Nilton Vieira (nvs@ecomp.poli.br)\n",
        "- Richard Jeremias (rjmr@ecomp.poli.br)\n",
        "\n",
        "\n",
        "**Etapas envolvidas**\n",
        "\n",
        "1. Recebimento da comunicação; ✅\n",
        "2. Pré-processamento da pergunta; ✅\n",
        "3. Geração do conjunto de atributos; ✅\n",
        "4. Classificação da pergunta (assunto); ✅\n",
        "5. Envio do resultado. ✅\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CovB-urKwnG_"
      },
      "source": [
        "**Instalação e imports iniciais**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-sB4f4L8J8r",
        "outputId": "2b7faad6-f62f-42af-a68e-54069ad2ac1c"
      },
      "source": [
        "#Instalação dos pacotes necessários\n",
        "!pip install seaborn\n",
        "!pip install spacy\n",
        "!pip install -U scikit-learn\n",
        "!pip install delayed\n",
        "!pip install -U nltk\n",
        "!python -m spacy download pt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.1)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2018.9)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.6.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.24.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (2.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: delayed in /usr/local/lib/python3.7/dist-packages (0.11.0b1)\n",
            "Requirement already satisfied: redis in /usr/local/lib/python3.7/dist-packages (from delayed) (3.5.3)\n",
            "Requirement already satisfied: hiredis in /usr/local/lib/python3.7/dist-packages (from delayed) (2.0.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.6.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.62.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Collecting pt_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.5/pt_core_news_sm-2.2.5.tar.gz (21.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.2 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from pt_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.6.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.10)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/pt_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/pt\n",
            "You can now load the model via spacy.load('pt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrcvOHbU4yIq"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMMNE_uuegzV",
        "outputId": "71e8ee31-d607-4f1a-b5c1-f35cf7e8ed55"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import pickle\n",
        "import pt_core_news_sm\n",
        "import spacy\n",
        "import re\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import RSLPStemmer\n",
        "from spacy.lang.pt.stop_words import STOP_WORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from collections import Counter\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "nltk.download('rslp') \n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X11Lx-GeAWyQ"
      },
      "source": [
        "**Importação e tratamento dos dados**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzJT1wj2eivn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "496173e4-f40d-4ab8-b53a-b4dec6662041"
      },
      "source": [
        "#IMPORTAÇÃO DA BASE DE DADOS \n",
        "filepath = 'BaseDados.csv'\n",
        "data = pd.read_csv(filepath, sep=',')\n",
        "print(len(data))\n",
        "data_filter = data[(data['TIPO RESPOSTA'] != 'Acesso negado')]\n",
        "print(len(data_filter))\n",
        "#tamanho do processamento = Quantidade de duplas\n",
        "tam_length = len(data_filter)\n",
        "\n",
        "#NORMALIZANDO AS PALAVRAS\n",
        "data_filter['pedido'] = data_filter['PEDIDO'].map(lambda x: x.lower())\n",
        "coluna_tmp = [0] * tam_length\n",
        "\n",
        "for i in range(tam_length):\n",
        "  # nova coluna inserida no step anterior\n",
        "  coluna_tmp[i] = data_filter.iloc[i]['pedido']\n",
        "  letra_sem_acento = \"a\"\n",
        "  coluna_tmp[i] = re.sub(r'([áàãâ])', letra_sem_acento, str(coluna_tmp[i]))\n",
        "  letra_sem_acento = \"e\"\n",
        "  coluna_tmp[i] = re.sub(r'([éê])', letra_sem_acento, str(coluna_tmp[i]))\n",
        "  letra_sem_acento = \"i\"\n",
        "  coluna_tmp[i] = re.sub(r'([í])', letra_sem_acento, str(coluna_tmp[i]))\n",
        "  letra_sem_acento = \"o\"\n",
        "  coluna_tmp[i] = re.sub(r'([óôõ])', letra_sem_acento, str(coluna_tmp[i]))\n",
        "  letra_sem_acento = \"u\"\n",
        "  coluna_tmp[i] = re.sub(r'([ú])', letra_sem_acento, str(coluna_tmp[i]))\n",
        "  letra_sem_acento = \"c\"\n",
        "  coluna_tmp[i] = re.sub(r'([ç])', letra_sem_acento, str(coluna_tmp[i]))\n",
        "\n",
        "data_filter['pedido'] = coluna_tmp\n",
        "\n",
        "# tirar caracteres especiais e os números\n",
        "coluna_tmp = [0] * tam_length\n",
        "for i in range(tam_length):\n",
        "    # nova coluna inserida no step anterior\n",
        "    coluna_tmp[i] = data_filter.iloc[i]['pedido']\n",
        "    letra_sem_acento = \"\"\n",
        "    coluna_tmp[i] = re.sub(r'([/\\\"-.,;:º@!?&%1234567890])',\n",
        "                           letra_sem_acento, str(coluna_tmp[i]))\n",
        "data_filter['pedido'] = coluna_tmp\n",
        "\n",
        "# tirar palavras com tamanho menor 2\n",
        "coluna_tmp = [0] * tam_length\n",
        "for i in range(tam_length):\n",
        "    tokens = str(data_filter.iloc[i]['pedido']).split()\n",
        "    word_tmp = \"\"\n",
        "    for word in tokens:\n",
        "        if len(word) > 2:\n",
        "            word_tmp = word_tmp + \" \" + word\n",
        "    coluna_tmp[i] = word_tmp\n",
        "\n",
        "data_filter['pedido'] = coluna_tmp\n",
        "\n",
        "nlp = pt_core_news_sm.load()\n",
        "stopwords = [k for k in STOP_WORDS]\n",
        "new_stopwords = []\n",
        "new_stopwords_list = stopwords.extend(new_stopwords)\n",
        "\n",
        "coluna_tmp = [0] * tam_length\n",
        "for i in range(tam_length):\n",
        "    doc = nlp(str(data_filter.iloc[i]['pedido']))\n",
        "\n",
        "    tokens = doc.text.split()\n",
        "\n",
        "    doc_temp = \"\"\n",
        "    for word in tokens:\n",
        "        if (word in stopwords) == False:\n",
        "            doc_temp = doc_temp + \" \" + word\n",
        "\n",
        "    coluna_tmp[i] = doc_temp\n",
        "\n",
        "data_filter['pedido'] = coluna_tmp\n",
        "resultado = data_filter\n",
        "resultado.to_csv('palavrasNormalizadas.csv')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3800\n",
            "3591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdpdcdsNirE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "190234ee-83bf-48bd-947a-9d6ac29202a1"
      },
      "source": [
        "#stemmer\n",
        "nlp = pt_core_news_sm.load()\n",
        "stemmer = RSLPStemmer()\n",
        "\n",
        "coluna_tmp = [0] * tam_length\n",
        "for i in range(tam_length):\n",
        "    doc = nlp(str(data_filter.iloc[i]['pedido']))\n",
        "\n",
        "    tokens = doc.text.split()\n",
        "\n",
        "    temp = \"\"\n",
        "    for token in tokens:\n",
        "        if token != \"nan\":\n",
        "            temp = temp + \" \" + stemmer.stem(token)\n",
        "\n",
        "    coluna_tmp[i] = temp.strip()\n",
        "data_filter['pedido'] = coluna_tmp\n",
        "print(data_filter['pedido'])\n",
        "# insertColumn(\"stremer\", coluna_tmp)\n",
        "data_filter['palavra'] = data_filter['pedido'].map(lambda x: x.lower()) \n",
        "#data_filter['pedido'].map(lambda x: x.lower()) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       solicit ouvid seguint informaco relaca contrat...\n",
            "1       relaca profes efet contrat are tecnolog inform...\n",
            "2       relaca profes regim contrat temporari lingu po...\n",
            "3       solicit relaca profes contrat municipi surubim...\n",
            "4       solicit relaca profes educaca fisic contrat ge...\n",
            "                              ...                        \n",
            "3791    solicit shapefil bac hidrograf rio tejipi curs...\n",
            "3794    bas lei estad art gost foss disponibil ato ren...\n",
            "3795    solicit seguint informaco secret educaca perna...\n",
            "3798    vist acess dad publ relacion process concessa ...\n",
            "3799    tardesolicit informaco pge atribuico competenc...\n",
            "Name: pedido, Length: 3591, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wmY8xsZmSqi"
      },
      "source": [
        "**Criação das colunas com as stepwords**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDmMN6her-fB"
      },
      "source": [
        "def saveStepWordsColumns(resultWithoutRepeat):\n",
        "  nameFileColumns = \"columns.csv\"\n",
        "  file = open(nameFileColumns, \"w\")\n",
        "  writer = csv.writer(file)\n",
        "  writer.writerow([item for item in resultWithoutRepeat])\n",
        "  return nameFileColumns\n",
        "\n",
        "resultadoSemRepeticao = set(data_filter['palavra'].str.cat(sep=',').replace(\" \",\",\").split(\",\"))\n",
        "resultadoSemRepeticao = set(pd.read_csv(saveStepWordsColumns(resultadoSemRepeticao), sep=','))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTmYM2WS3_bH"
      },
      "source": [
        "### **Manipulação de número de amostras para treinamento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpOIHGkn4J3w"
      },
      "source": [
        "numeroAmostras = len(data_filter['ASSUNTO'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWcwkykTmPAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38de3b59-d3e7-42ab-ae70-1ca641a7b358"
      },
      "source": [
        "corpus = data_filter['palavra']\n",
        "print(corpus)\n",
        "def get_top_n_words(corpus, n=None, nWords = 3): #nWords define a quantidade de palavras armazenadas por perguntas. Por default, 3\n",
        "    vec3 = CountVectorizer(ngram_range=(\n",
        "        nWords, nWords), max_features=10000,  max_df=0.8).fit(corpus)\n",
        "    bag_of_words = vec3.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx])\n",
        "                  for word, idx in vec3.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "top3_words = get_top_n_words(corpus, n=numeroAmostras, nWords = 5)\n",
        "top3_df = pd.DataFrame(top3_words)\n",
        "top3_df.columns = [\"Tri-gram\", \"Freq\"]\n",
        "\n",
        "resultado = top3_df\n",
        "resultado.to_csv('topNWords_cv')\n",
        "\n",
        "# plotar\n",
        "def plotar(nomeX, nomeY, dados, filename='grafico.png'):\n",
        "    sns.set(rc={'figure.figsize': (20, 15)})\n",
        "    g = sns.barplot(y=nomeY, x=nomeX, data=dados)\n",
        "    g.tick_params(labelsize=8)\n",
        "    g.set_xlabel(nomeY, fontsize=10)\n",
        "    g.set_ylabel(nomeX, fontsize=10)\n",
        "    plt.savefig(filename)\n",
        "    return\n",
        "\n",
        "#plotar(nomeX=\"Freq\", nomeY=\"Word\", dados=top_df, filename='grafico1.png')\n",
        "#plotar(nomeX=\"Freq\", nomeY=\"Bi-gram\", dados=top2_df, filename='grafico2.png')\n",
        "#plotar(nomeX=\"Freq\", nomeY=\"Tri-gram\", dados=top3_df, filename='grafico3.png')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       solicit ouvid seguint informaco relaca contrat...\n",
            "1       relaca profes efet contrat are tecnolog inform...\n",
            "2       relaca profes regim contrat temporari lingu po...\n",
            "3       solicit relaca profes contrat municipi surubim...\n",
            "4       solicit relaca profes educaca fisic contrat ge...\n",
            "                              ...                        \n",
            "3791    solicit shapefil bac hidrograf rio tejipi curs...\n",
            "3794    bas lei estad art gost foss disponibil ato ren...\n",
            "3795    solicit seguint informaco secret educaca perna...\n",
            "3798    vist acess dad publ relacion process concessa ...\n",
            "3799    tardesolicit informaco pge atribuico competenc...\n",
            "Name: palavra, Length: 3591, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hJIPth817O4"
      },
      "source": [
        "# **Descobrindo o número de vezes que o mesmo assunto aparece**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OYkcIMK16VK"
      },
      "source": [
        "listaAssunto = [x.lower() for x in set(data_filter['ASSUNTO'])]\n",
        "dicionario = {'pedido' :  data_filter['pedido'], 'assunto' : [x.lower() for x in data_filter['ASSUNTO']]} #lower nos assuntos\n",
        "\n",
        "listaOutros = set() \n",
        "\n",
        "def lowOccurrenceTopics(totalTopics, rate = 1): #método detecta baixa ocorrência de tópicos inferiores a taxa 'rate'\n",
        "  othersSet = set()\n",
        "  for key, tamanho in totalTopics.items():\n",
        "    if ((tamanho/len(data_filter))*100) < rate: #tamanho de inputs de data_filter agora pode ser variável\n",
        "      othersSet.add(key)\n",
        "  return othersSet\n",
        "\n",
        "def updateDataSetLabels(finalTopicList, topicsList, labelName = \"outros\"): #método responsável por aplicar as mudanças nos rótulos no dicionáro composto pela chave 'pedido' e 'assunto'\n",
        "  count = 0\n",
        "  for item in finalTopicList:\n",
        "    if(item in topicsList):\n",
        "      finalTopicList[count] = labelName\n",
        "    count = count + 1\n",
        "  return finalTopicList\n",
        "\n",
        "listaOutros = lowOccurrenceTopics(dict(Counter(dicionario['assunto'])), rate = 8)\n",
        "#print(listaOutros)\n",
        "#atualização de rótulos feita direto em dicionário que contém pedido -> assunto\n",
        "#print(dicionario['assunto'])\n",
        "dicionario['assunto'] = updateDataSetLabels(dicionario['assunto'], listaOutros, 'outros')\n",
        "\n",
        "#print(dicionario['assunto'])\n",
        "\n",
        "#UTILIZAR \n",
        "#listaAssunto = dict(Counter(listaAssunto)) #obtenção de ocorrência de assuntos\n",
        "#listaAssunto = {k: v for k, v in sorted(listaAssunto.items(), key=lambda item: item[1])} #ordenamento crescente por quantidade\n",
        "#print(listaAssunto)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yJNipPjbg6J"
      },
      "source": [
        "## **Balancear assuntos**\n",
        "Deixar o número de corrências dos assuntos o mais próximo possível, para não haver uma descrepância de alguns assuntos com muitas ocorrências e outros com poucas ocorrências."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaGAUGRxbyRE",
        "outputId": "ba11d667-dcf0-4aca-9583-34ce5643315e"
      },
      "source": [
        "print(dict(Counter(dicionario['assunto'])))\n",
        "\n",
        "#Salvar assuntos\n",
        "def saveTopics(topics):\n",
        "  file = open('topics.csv', \"w\")\n",
        "  writer = csv.writer(file)\n",
        "  print(set(topics))\n",
        "  writer.writerow([item for item in set(topics)])\n",
        "\n",
        "saveTopics(dicionario['assunto'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'servidores': 860, 'outros': 1510, 'execução orçamentária e financeira': 379, 'licitações e contratos': 429, 'segurança pública': 413}\n",
            "{'execução orçamentária e financeira', 'outros', 'servidores', 'segurança pública', 'licitações e contratos'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rrlQg6FQDrb"
      },
      "source": [
        "# **Binarização das stepwords**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOp61W6zofjL"
      },
      "source": [
        "nameFileColumns = \"columns.csv\"\n",
        "file = open(nameFileColumns, \"w\")\n",
        "writer = csv.writer(file)\n",
        "\n",
        "data = list(resultado['Tri-gram'])\n",
        "last_data = [item.split() for item in data]\n",
        "\n",
        "listaAssuntos = dicionario['assunto']\n",
        "\n",
        "listaTotal = []\n",
        "listaTeste = []\n",
        "index = 0\n",
        "for n_palavras_perguntas in last_data:\n",
        "  listaParcial = []\n",
        "  for cada_coluna in resultadoSemRepeticao:\n",
        "    if cada_coluna in n_palavras_perguntas:\n",
        "      listaParcial.append(1) \n",
        "    elif cada_coluna is None:\n",
        "      listaParcial.append(None)\n",
        "    else:\n",
        "      listaParcial.append(0)\n",
        "  if index < int(numeroAmostras*0.8):\n",
        "    listaTotal.append(listaParcial)\n",
        "  else:\n",
        "    listaTeste.append(listaParcial)\n",
        "  index = index + 1\n",
        "\n",
        "for x in listaTotal:\n",
        "  writer.writerow([item for item in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn1v-DhjZXOo"
      },
      "source": [
        "# **Divisão em dados de testes e treinamento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81iYIMzFZdLn"
      },
      "source": [
        "#divisão dos dados em 20% para testes e 80% para treinamento\n",
        "x, y = None, None\n",
        "x = coluna_tmp\n",
        "X_train, X_test, y_train, y_test = None, None, None, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RXVkh5ULwX8"
      },
      "source": [
        "Pré-processamento de input do usuário"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr_o4Pe-LvNf"
      },
      "source": [
        "def proccessText(text): #chamar este método para pré-processar pergunta\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'([áàãâ])', 'a', str(text))\n",
        "  text = re.sub(r'([éê])', 'e', str(text))\n",
        "  text = re.sub(r'([í])', 'i', str(text))\n",
        "  text = re.sub(r'([óôõ])', 'o', str(text))\n",
        "  text = re.sub(r'([ú])', 'u', str(text))\n",
        "  text = re.sub(r'([ç])', 'c', str(text))\n",
        "  # tirar caracteres especiais e os números\n",
        "  text = re.sub(r'([/\\\"-.,;:º@!?&%1234567890])',\"\", str(text))\n",
        "  \n",
        "  # tirar palavras com tamanho menor 2\n",
        "  coluna_tmp = [0] * len(text)\n",
        "  tokens = str(text).split()\n",
        "  word_tmp = \"\"\n",
        "  for word in tokens:\n",
        "    if len(word) > 3:\n",
        "      word_tmp = word_tmp + \" \" + word\n",
        "  text = word_tmp\n",
        "\n",
        "  nlp = pt_core_news_sm.load()\n",
        "  stopwords = [k for k in STOP_WORDS]\n",
        "  new_stopwords = []\n",
        "  new_stopwords_list = stopwords.extend(new_stopwords)\n",
        "  doc = nlp(str(text))\n",
        "  tokens = doc.text.split()\n",
        "\n",
        "  doc_temp = \"\"\n",
        "  for word in tokens:\n",
        "    if (word in stopwords) == False:\n",
        "      doc_temp = doc_temp + \" \" + word\n",
        "  text = doc_temp\n",
        "  return pergunta(text)\n",
        "\n",
        "\n",
        "def binarizeInput(text, wordList): #como binarizar input???\n",
        "  binaryList = []\n",
        "  for word in wordList:\n",
        "    if(word in text):\n",
        "      binaryList.append(1)\n",
        "    else:\n",
        "      binaryList.append(0)\n",
        "  return binaryList\n",
        "\n",
        "def feedback():\n",
        "  \n",
        "  pass\n",
        "\n",
        "def pergunta(text):\n",
        "  text = proccessText(text)\n",
        "  doc = nlp(str(text))\n",
        "  tokens = doc.text.split()\n",
        "  temp = \"\"\n",
        "  for token in tokens:\n",
        "    if token != \"nan\":\n",
        "      temp = temp + \" \" + stemmer.stem(token)\n",
        "  return temp\n",
        "\n",
        "#print(proccessText(\"olá eu não gostaria matar criança feminicídio solicito de saber essa informação\"))\n",
        "#nojeira = \"olá eu não gostaria matar criança feminicídio solicito de saber essa informação\"\n",
        "#aux = pergunta(nojeira)\n",
        "#binary = binarizeInput(aux, resultadoSemRepeticao)\n",
        "#binary = pergunta(proccessText(\"olá eu não gostaria matar criança feminicídio solicito de saber essa informação\"), resultadoSemRepeticao)\n",
        "#print(binary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c8tpp9GZfFh"
      },
      "source": [
        "# **Treinamento e predição com dados de testes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmVKPiMjF5R0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c100aa8-19d4-4c30-eea6-48f94adaa482"
      },
      "source": [
        "!pip install imbalanced-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (0.24.2)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->imbalanced-learn) (2.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->imbalanced-learn) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4x5_u-0ZhUt"
      },
      "source": [
        "#uso de random forest algorithm pra treinar modelo\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn import tree\n",
        "#from imblearn.under_sampling import RandomUnderSampler \n",
        "\n",
        "def train():\n",
        "  X, y = listaTotal,listaAssuntos[:int(numeroAmostras*0.8)]\n",
        "  #clf = tree.DecisionTreeClassifier() #32%\n",
        "  #clf = clf.fit(X, y)\n",
        "  #tree.plot_tree(clf)\n",
        "\n",
        "  #clf = RandomForestClassifier(max_depth=10, random_state=0)\n",
        "  #clf.fit(X, y)\n",
        "  # usar técnica under-sampling\n",
        "  clf = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=5, bootstrap = True,  random_state=0) #50%\n",
        "  #rus = RandomUnderSampler()\n",
        "  #X_res, y_res = rus.fit_sample(X, y)\n",
        "  #clf.fit(X_res, y_res)\n",
        "  clf.fit(X, y)\n",
        "\n",
        "  #clf = ExtraTreesClassifier(n_estimators=100, random_state=0) #34%\n",
        "  #clf.fit(X, y)\n",
        "  return clf\n",
        "\n",
        "clf = train()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwD9E49VZh8M"
      },
      "source": [
        "# **Status do modelo treinado**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wcMK9WzTmeQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ca06e57-e249-4929-b8f9-2653c20a6e37"
      },
      "source": [
        "y_pred = []\n",
        "X_teste,y_teste = listaTeste,listaAssuntos[(int(numeroAmostras*0.8)):numeroAmostras]  \n",
        "print(clf.score(X_teste,y_teste))\n",
        "for index in range(len(y_teste)):\n",
        "  value = clf.predict([X_teste[index]])\n",
        "  y_pred.append(value)\n",
        "\n",
        "#Salvar assuntos\n",
        "def saveResults(x, y): #passar lista de perguntas antes de binarizar....\n",
        "  file = open('results.csv', \"w\")\n",
        "  writer = csv.writer(file)\n",
        "  if(len(x)==len(y)): #deve existir o par (x,y)\n",
        "    with open('results.txt', 'w') as f:\n",
        "      f.write('PEDIDO   ASSUNTO\\n')\n",
        "      for index in range(len(x)):\n",
        "        f.write(f'{x[index]}      {y[index]};\\n')\n",
        "    \n",
        "    \n",
        "saveResults(['1', '2', '3'] ,['4', '5', '6']) #exemplo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5048678720445062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ICSeiJpL4HO"
      },
      "source": [
        "# **Interface de comunicação**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjT5L52fL0-5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RZkf1i3L9RP"
      },
      "source": [
        "!pip install Flask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_AyZQsLL9Lj"
      },
      "source": [
        "from flask import Flask, jsonify\n",
        "\n",
        "#criada instancia da classe\n",
        "app = Flask(__name__)\n",
        "\n",
        "#-----------INTERAÇÃO COM O FRONT\n",
        "#recebimento da comunicação\n",
        "@app.route(\"/\", methods = ['POST'])\n",
        "def getInfo():\n",
        "  #inicio do processamento,\n",
        "  #aqui deve ser enviado o resultado quando estiver disponível\n",
        "  #criar método predict\n",
        "  #avaliar json recebido...\n",
        "  return jsonify(\n",
        "      tema = \"tema x\"\n",
        "  )\n",
        "\n",
        "\n",
        "#classificar novamente se necessário\n",
        "@app.route(\"/send_feedback\", methods = ['POST'])\n",
        "def getFeedback():\n",
        "  return \"feedback received...\"\n",
        "\n",
        "#envio dos assuntos para equipe 4 e 5, retorna lista de assuntos processada\n",
        "@app.route(\"/get_topics\", methods = ['GET'])\n",
        "def getTopics():\n",
        "  topics = list(pd.read_csv('topics.csv'))\n",
        "  return jsonify(topics)\n",
        "\n",
        "\n",
        "#-----------INTERAÇÃO COM AS DEMAIS ENGINES\n",
        "app.run(port=5100) #aplicação disponível na porta: 5100, ip: localhost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx8VjDHz_E6D"
      },
      "source": [
        "**Chaves JSON**\n",
        "\n",
        "\n",
        "descricao_tratada, resposta_publica, unidade_gestora, assunto"
      ]
    }
  ]
}